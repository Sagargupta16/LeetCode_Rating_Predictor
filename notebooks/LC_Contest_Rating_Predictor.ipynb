{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP891Zwc4h/QxO5uTRB0lwp",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sagargupta16/LeetCode_Rating_Predictor/blob/main/LC_Contest_Rating_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nimport tensorflow as tf\nimport joblib\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nROOT = Path(\"..\") if Path(\"../data/data.json\").exists() else Path(\".\")\nDATA_PATH = ROOT / \"data\" / \"data.json\"\nMODEL_PATH = ROOT / \"model.keras\"\nSCALER_PATH = ROOT / \"scaler.save\"\nNUM_FEATURES = 15\n\nprint(f\"TensorFlow {tf.__version__}\")\ngpus = tf.config.list_physical_devices(\"GPU\")\nif gpus:\n    print(f\"GPU: {gpus[0].name}\")\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"No GPU — training on CPU (use WSL2 for GPU)\")\nprint(f\"Data: {DATA_PATH.resolve()}\")",
   "metadata": {
    "id": "r9DxuNQ9r_vI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "a20ac844-5905-489c-c58b-0d1e3e0b4b25"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "UKlWkOAAlK5q"
   },
   "outputs": [],
   "source": "## Data Collection\n\n**Skip cells 2-4 if you already have `data/data.json`.**\n\nTo refresh training data, run from the project root:\n```bash\npython scripts/update_data.py\n```\nThe REST contest API is now blocked (403), so data collection uses GraphQL via `scripts/update_data.py`."
  },
  {
   "cell_type": "markdown",
   "source": "*(Data collection cells removed -- use `python scripts/update_data.py` instead)*",
   "metadata": {
    "id": "rI0pFAlC0DpZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load and Prepare Data — 15 Features\n\n| # | Key | Feature | Correlation |\n|---|-----|---------|-------------|\n| 1 | f1 | current_rating | -0.124 |\n| 2 | f2 | rank | **-0.339** |\n| 3 | f3 | total_participants | -0.159 |\n| 4 | f4 | rank_percentage | **-0.374** |\n| 5 | f5 | attended_contests | -0.140 |\n| 6 | f6 | avg_solve_rate (all-time) | -0.055 |\n| 7 | f7 | avg_finish_time (all-time) | +0.091 |\n| 8 | f8 | recent_solve_rate (last 5) | -0.086 |\n| 9 | f9 | recent_finish_time (last 5) | +0.066 |\n| 10 | f10 | rating_trend (last 5 changes) | +0.031 |\n| 11 | f11 | max_rating | -0.118 |\n| 12 | f12 | log(1 + rank) | **-0.432** |\n| 13 | f13 | rating * percentile | **-0.432** |\n| 14 | f14 | avg_solve_rate * rating | -0.094 |\n| 15 | f15 | time_efficiency (avg_ft / 5400) | +0.091 |",
   "metadata": {
    "id": "p4SQazsrrQ4J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6f90a829-fa78-4e4f-9545-471e16d62ba9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load data (15 features: f1-f15 + output)\nrecords = []\nwith open(DATA_PATH) as f:\n    for line in f:\n        records.append(json.loads(line))\nprint(f\"Loaded {len(records)} records\")\n\n# Build feature matrix — order must match production main.py\nX = np.array([[r[f\"f{i}\"] for i in range(1, NUM_FEATURES + 1)] for r in records])\ny = np.array([r[\"output\"] for r in records])\n\nassert X.shape[1] == NUM_FEATURES, f\"Expected {NUM_FEATURES} features, got {X.shape[1]}\"\nprint(f\"Features: {X.shape}, Output: mean={y.mean():.2f}, std={y.std():.2f}\")\n\n# Scale\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\njoblib.dump(scaler, SCALER_PATH)\nprint(f\"Scaler saved ({NUM_FEATURES} features)\")\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=SEED)\nprint(f\"Train: {X_train.shape[0]:,}, Test: {X_test.shape[0]:,}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGW-Rsbs1Ydm",
    "outputId": "65a788af-45a2-47bc-e427-8eeca7076f36"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Model: Dense(128)->Dropout(0.3)->Dense(64)->Dropout(0.2)->Dense(32)->Dense(1)\nmodel = Sequential([\n    Dense(128, activation=\"relu\", input_shape=(NUM_FEATURES,)),\n    Dropout(0.3),\n    Dense(64, activation=\"relu\"),\n    Dropout(0.2),\n    Dense(32, activation=\"relu\"),\n    Dense(1),\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\nmodel.summary()\n\nearly_stop = EarlyStopping(\n    monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.1,\n    epochs=300,\n    batch_size=128,\n    callbacks=[early_stop],\n    verbose=2,\n)\n\ntest_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nTest MSE:  {test_loss:.2f}\")\nprint(f\"Test RMSE: {np.sqrt(test_loss):.2f}\")\nprint(f\"Test MAE:  {test_mae:.2f}\")\n\n# Save main model + versioned backup\nmodel.save(MODEL_PATH)\nprint(f\"Model saved to {MODEL_PATH}\")\n\nfrom datetime import datetime\nversion = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nversioned_dir = ROOT / \"models\" / \"history\"\nversioned_dir.mkdir(parents=True, exist_ok=True)\nmodel.save(versioned_dir / f\"model_{version}.keras\")\njoblib.dump(scaler, versioned_dir / f\"scaler_{version}.save\")\nwith open(versioned_dir / f\"metrics_{version}.json\", \"w\") as mf:\n    import json as _json\n    _json.dump({\"mse\": float(test_loss), \"rmse\": float(np.sqrt(test_loss)),\n                \"mae\": float(test_mae), \"records\": len(records),\n                \"features\": NUM_FEATURES, \"params\": model.count_params()}, mf, indent=2)\nprint(f\"Versioned backup: models/history/*_{version}.*\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5RLpNU0wVLm",
    "outputId": "9e94cd83-cb0c-42c4-881e-cf7ab52f84fe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sanity check\nimport math\nrating, rank, participants, attended = 1957, 1869, 21165, 87\npct = (rank * 100) / participants\nsample = np.array([[\n    rating, rank, participants, pct, attended,       # f1-f5\n    0.58, 3100, 0.45, 2200, 4.0, 2007,              # f6-f11 (example user history)\n    math.log1p(rank), rating*(rank/participants),     # f12-f13\n    0.58*rating, 3100/5400,                           # f14-f15\n]])\npred = model.predict(scaler.transform(sample), verbose=0)\nprint(f\"Rating: {rating}, Rank: {rank}/{participants}\")\nprint(f\"Predicted change: {pred[0][0]:+.2f}\")\nprint(f\"Predicted new rating: {rating + pred[0][0]:.2f}\")\n\n# Training curve\ntry:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    ax1.plot(history.history[\"loss\"], label=\"Train\")\n    ax1.plot(history.history[\"val_loss\"], label=\"Val\")\n    ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"MSE\"); ax1.set_title(\"Loss\"); ax1.legend()\n    ax2.plot(history.history[\"mae\"], label=\"Train\")\n    ax2.plot(history.history[\"val_mae\"], label=\"Val\")\n    ax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"MAE\"); ax2.set_title(\"MAE\"); ax2.legend()\n    plt.tight_layout(); plt.show()\nexcept ImportError:\n    print(\"matplotlib not installed, skipping plot\")",
   "metadata": {
    "id": "YCtog77Yesm4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "513b5d75-052a-4a9c-f437-a62bf4a7e6da"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}