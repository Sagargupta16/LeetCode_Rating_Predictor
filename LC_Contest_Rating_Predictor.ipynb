{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6Ce84wEZ/3Q0x2M21UoMy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagargupta16/LeetCode_Rating_Predictor/blob/main/LC_Contest_Rating_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning Model Preparation and Execution Libraries\n",
        "# This script includes imports for handling data, building and training machine learning models,\n",
        "# performing numerical computations, and parallel processing.\n",
        "\n",
        "import json  # For parsing and handling JSON data\n",
        "import numpy as np  # For numerical operations, mainly with arrays\n",
        "import pandas as pd  # For data manipulation and analysis, especially with tabular data\n",
        "import requests  # For making HTTP requests to web servers\n",
        "import tensorflow as tf  # For building and training machine learning models\n",
        "import time  # For working with time-related functions\n",
        "from sklearn.model_selection import train_test_split  # For splitting datasets into training and testing sets\n",
        "from sklearn.preprocessing import MinMaxScaler  # For feature scaling, specifically min-max normalization\n",
        "from tensorflow.keras.models import Sequential  # For creating a linear stack of neural network layers\n",
        "from tensorflow.keras.layers import Dense, LSTM  # Dense for fully connected layers, LSTM for Long Short-Term Memory layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping  # Callbacks for saving models and early stopping\n",
        "from concurrent.futures import ThreadPoolExecutor  # For executing calls asynchronously\n",
        "import joblib  # For saving and loading Python objects that make use of NumPy data structures"
      ],
      "metadata": {
        "id": "r9DxuNQ9r_vI"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UKlWkOAAlK5q"
      },
      "outputs": [],
      "source": [
        "# Define GraphQL Query and Headers\n",
        "\n",
        "# GraphQL query for fetching user contest data.\n",
        "# The query requests the contest ranking history of a user including fields like\n",
        "# attendance status, rating, ranking, and contest title.\n",
        "# It takes a username as a variable ($username).\n",
        "query = \"\"\"\n",
        "query userContestRankingInfo($username: String!) {\n",
        "    userContestRankingHistory(username: $username) {\n",
        "        attended\n",
        "        rating\n",
        "        ranking\n",
        "        contest {\n",
        "            title\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Headers for the GraphQL request.\n",
        "# \"Content-Type: application/json\" indicates that the request body format is JSON,\n",
        "# which is a common requirement for GraphQL APIs.\n",
        "headers = {\"Content-Type\": \"application/json\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LeetCode Contest Data Fetching Functions\n",
        "\n",
        "# Session for making HTTP requests\n",
        "session = requests.Session()\n",
        "\n",
        "# Fetch the count of users participating in a given contest\n",
        "def fetch_contest_users_count(contest_title):\n",
        "    url = f\"https://leetcode.com/contest/api/ranking/{contest_title}/\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {\"error\": \"Failed to fetch data\"}\n",
        "\n",
        "# Read a specified number of usernames from a JSON file\n",
        "def read_usernames_from_json(file_path, number_of_usernames=500):\n",
        "    with open(file_path, 'r') as file:\n",
        "        all_usernames = json.load(file)\n",
        "        return all_usernames[:number_of_usernames]\n",
        "\n",
        "# Fetch contest ranking history data for a given username\n",
        "def fetch_data(username):\n",
        "    response = requests.post(\n",
        "        \"https://leetcode.com/graphql\",\n",
        "        headers=headers,\n",
        "        json={\"query\": query, \"variables\": {\"username\": username}}\n",
        "    )\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"data\", {}).get(\"userContestRankingHistory\", [])\n",
        "    else:\n",
        "        print(f\"Error fetching data for username {username}: {response.status_code}\")\n",
        "        return []  # Return an empty list in case of error"
      ],
      "metadata": {
        "id": "rI0pFAlC0DpZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process user contest data to generate a structured dataset\n",
        "def process_data(contests):\n",
        "    data = []\n",
        "    rating = 1500\n",
        "    z = 0\n",
        "    for contest in contests:\n",
        "        if contest[\"attended\"]:\n",
        "            rating, ranking = contest[\"rating\"], contest[\"ranking\"]\n",
        "            contest_title = contest[\"contest\"][\"title\"].lower().replace(\" \", \"-\")\n",
        "            if \"weekly\" in contest_title and contest_title.split(\"-\")[-1].isdigit():\n",
        "                contest_title = \"leetcode-\" + contest_title\n",
        "            contest_title = \"weekly-contest-by-app-academy\" if contest_title == \"weekly-contest-62\" else contest_title\n",
        "\n",
        "            if contest_title not in contest_participants:\n",
        "                x = fetch_contest_users_count(contest_title)\n",
        "                contest_participants[contest_title] = x.get(\"user_num\", 0)\n",
        "\n",
        "            total_participants = contest_participants[contest_title]\n",
        "            data.append([rating, ranking, total_participants, (ranking / total_participants) * 100 if total_participants else 0, z, rating - rating])\n",
        "            z += 1\n",
        "    return data\n",
        "\n",
        "# Process a batch of usernames in parallel to fetch and process their contest data\n",
        "def process_batch_parallel(usernames, max_workers=100):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = executor.map(fetch_data, usernames)\n",
        "    return [item for sublist in map(process_data, results) if sublist for item in sublist]\n",
        "\n",
        "# Main script: Read usernames and process their data in batches\n",
        "usernames = read_usernames_from_json('usernames.json')\n",
        "all_data = []\n",
        "\n",
        "for i in range(0, len(usernames), 250):\n",
        "    all_data.extend(process_batch_parallel(usernames[i:i + 250]))\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "p4SQazsrrQ4J"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the aggregated data into a DataFrame and output it\n",
        "df = pd.DataFrame(all_data, columns=['input1', 'input2', 'input3' , 'input4' ,'input5' , 'output'])\n",
        "print(df)\n",
        "\n",
        "# Save the DataFrame to a JSON file\n",
        "df.to_json('data.json', orient='records', lines=True)\n",
        "\n",
        "# Separate the features (X) and target variable (y)\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df['output'].values\n",
        "\n",
        "# Apply Min-Max scaling to the features for normalization\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Save the scaler for future use\n",
        "joblib.dump(scaler, 'scaler.save')\n",
        "\n",
        "# Reshape X for compatibility with machine learning models (if needed)\n",
        "X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGW-Rsbs1Ydm",
        "outputId": "a3914e4b-40ef-4551-8187-9cb011483227"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         input1  input2  input3     input4  input5   output\n",
            "0      1500.000     110   13038   0.843688       0  320.626\n",
            "1      1820.626      45   11877   0.378884       1  199.425\n",
            "2      2020.051      69    7926   0.870553       2  102.787\n",
            "3      2122.838     119   13283   0.895882       3   73.217\n",
            "4      2196.055     384   13805   2.781601       4   19.398\n",
            "...         ...     ...     ...        ...     ...      ...\n",
            "10773  1454.323    5098   23404  21.782601       3   46.008\n",
            "10774  1500.000     328    6631   4.946464       0  215.392\n",
            "10775  1715.392    1027    7873  13.044583       1   44.149\n",
            "10776  1759.541    1376   17435   7.892171       2   41.787\n",
            "10777  1801.328    2413   26069   9.256205       3   28.621\n",
            "\n",
            "[10778 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure using a GPU if available for model training\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    # Define an LSTM model. The model architecture includes an LSTM layer followed by a Dense layer.\n",
        "    # The activation functions are set to 'leaky_relu' for the LSTM and default (linear) for the Dense layer.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(100, activation='leaky_relu', recurrent_activation='sigmoid', input_shape=(1, 5)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Define the learning rate for the optimizer\n",
        "    learning_rate = 0.01\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model with Adam optimizer and mean squared error loss function\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Train the model using the training dataset\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
        "\n",
        "    # Evaluate the model's performance on the test dataset\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(\"Test Loss:\", loss)\n",
        "\n",
        "    # Save the trained model to a file for later use\n",
        "    model.save('model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5RLpNU0wVLm",
        "outputId": "2714c2e3-ed27-40c6-be46-8e5dd6e8f841"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "236/236 [==============================] - 3s 6ms/step - loss: 1054.1334\n",
            "Epoch 2/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 547.6153\n",
            "Epoch 3/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 435.4152\n",
            "Epoch 4/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 387.7589\n",
            "Epoch 5/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 356.9578\n",
            "Epoch 6/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 330.1986\n",
            "Epoch 7/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 303.8665\n",
            "Epoch 8/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 282.6045\n",
            "Epoch 9/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 264.1165\n",
            "Epoch 10/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 247.4189\n",
            "Epoch 11/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 232.4599\n",
            "Epoch 12/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 216.6178\n",
            "Epoch 13/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 207.7817\n",
            "Epoch 14/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 194.5804\n",
            "Epoch 15/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 183.4731\n",
            "Epoch 16/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 174.9890\n",
            "Epoch 17/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 166.7896\n",
            "Epoch 18/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 159.3571\n",
            "Epoch 19/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 154.2542\n",
            "Epoch 20/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 148.1954\n",
            "Epoch 21/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 144.8107\n",
            "Epoch 22/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 139.4619\n",
            "Epoch 23/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 136.8016\n",
            "Epoch 24/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 132.9417\n",
            "Epoch 25/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 129.6042\n",
            "Epoch 26/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 127.8738\n",
            "Epoch 27/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 125.0542\n",
            "Epoch 28/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 123.1866\n",
            "Epoch 29/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 121.6949\n",
            "Epoch 30/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 118.7299\n",
            "Epoch 31/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 117.1618\n",
            "Epoch 32/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 117.0960\n",
            "Epoch 33/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 114.6755\n",
            "Epoch 34/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 112.5964\n",
            "Epoch 35/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 111.8591\n",
            "Epoch 36/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 110.8949\n",
            "Epoch 37/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 110.6941\n",
            "Epoch 38/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 108.8752\n",
            "Epoch 39/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 107.8813\n",
            "Epoch 40/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 106.0623\n",
            "Epoch 41/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 106.0008\n",
            "Epoch 42/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 105.4850\n",
            "Epoch 43/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 102.6926\n",
            "Epoch 44/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 100.9632\n",
            "Epoch 45/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 100.4359\n",
            "Epoch 46/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 100.2752\n",
            "Epoch 47/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 98.1667\n",
            "Epoch 48/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 99.2393\n",
            "Epoch 49/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 97.2108\n",
            "Epoch 50/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 96.9842\n",
            "Epoch 51/100\n",
            "236/236 [==============================] - 2s 6ms/step - loss: 96.6102\n",
            "Epoch 52/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 94.1321\n",
            "Epoch 53/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 93.7005\n",
            "Epoch 54/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 93.9278\n",
            "Epoch 55/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 91.4113\n",
            "Epoch 56/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 92.4170\n",
            "Epoch 57/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 91.2051\n",
            "Epoch 58/100\n",
            "236/236 [==============================] - 2s 9ms/step - loss: 90.4075\n",
            "Epoch 59/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 89.4057\n",
            "Epoch 60/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 88.8555\n",
            "Epoch 61/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 88.0748\n",
            "Epoch 62/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 89.8908\n",
            "Epoch 63/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 87.0284\n",
            "Epoch 64/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 88.2613\n",
            "Epoch 65/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 85.9449\n",
            "Epoch 66/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 85.3252\n",
            "Epoch 67/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 86.4387\n",
            "Epoch 68/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 84.3624\n",
            "Epoch 69/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 84.8604\n",
            "Epoch 70/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 84.1220\n",
            "Epoch 71/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 82.5717\n",
            "Epoch 72/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 83.7275\n",
            "Epoch 73/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 82.8654\n",
            "Epoch 74/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 82.0380\n",
            "Epoch 75/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 81.0100\n",
            "Epoch 76/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 80.7458\n",
            "Epoch 77/100\n",
            "236/236 [==============================] - 2s 7ms/step - loss: 80.7947\n",
            "Epoch 78/100\n",
            "236/236 [==============================] - 2s 8ms/step - loss: 80.4449\n",
            "Epoch 79/100\n",
            "236/236 [==============================] - 4s 16ms/step - loss: 80.3193\n",
            "Epoch 80/100\n",
            "236/236 [==============================] - 2s 9ms/step - loss: 80.6227\n",
            "Epoch 81/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 79.4590\n",
            "Epoch 82/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 78.9611\n",
            "Epoch 83/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 77.8185\n",
            "Epoch 84/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 78.1846\n",
            "Epoch 85/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 77.2752\n",
            "Epoch 86/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 77.2358\n",
            "Epoch 87/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 76.9329\n",
            "Epoch 88/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 76.5417\n",
            "Epoch 89/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 76.1833\n",
            "Epoch 90/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 75.5561\n",
            "Epoch 91/100\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 74.5619\n",
            "Epoch 92/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 75.2649\n",
            "Epoch 93/100\n",
            "236/236 [==============================] - 1s 6ms/step - loss: 74.1429\n",
            "Epoch 94/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 74.1697\n",
            "Epoch 95/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 73.6056\n",
            "Epoch 96/100\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 74.3241\n",
            "Epoch 97/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 72.9266\n",
            "Epoch 98/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 73.2363\n",
            "Epoch 99/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 72.7551\n",
            "Epoch 100/100\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 71.3804\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 104.4581\n",
            "Test Loss: 104.4581298828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import requests\n",
        "\n",
        "# Load the trained model and the scaler\n",
        "model = tf.keras.models.load_model('/content/model.keras')\n",
        "scaler = joblib.load('/content/scaler.save')\n",
        "\n",
        "# GraphQL query for fetching user contest data\n",
        "query = \"\"\"\n",
        "query userContestRankingInfo($username: String!) {\n",
        "        userContestRanking(username: $username) {\n",
        "            attendedContestsCount\n",
        "            rating\n",
        "        }\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "# Headers for the GraphQL request\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Fetch data for a given username using GraphQL\n",
        "def fetch_data(username):\n",
        "    response = requests.post(\n",
        "        \"https://leetcode.com/graphql\",\n",
        "        headers=headers,\n",
        "        json={\"query\": query, \"variables\": {\"username\": username}}\n",
        "    )\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"data\", {}).get(\"userContestRanking\", [])\n",
        "    else:\n",
        "        print(f\"Error fetching data for username {username}: {response.status_code}\")\n",
        "        return []  # Return an empty list in case of error\n",
        "\n",
        "# Prompt the user for input and fetch necessary data\n",
        "def get_user_input():\n",
        "    print(\"Enter the input values:\")\n",
        "    username = input(\"Enter your username: \")\n",
        "    data = fetch_data(username)\n",
        "    input1 = data[\"rating\"]\n",
        "    input2 = int(input(\"Enter your ranking: \"))\n",
        "    input3 = int(input(\"Enter Total Participants: \"))\n",
        "    input4 = (input2*100) / input3 if input3 != 0 else 0\n",
        "    input5 = data[\"attendedContestsCount\"]\n",
        "    return np.array([[input1, input2, input3, input4, input5]])\n",
        "\n",
        "# Normalize and make a prediction based on user input\n",
        "def make_prediction(input_data):\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    input_scaled = input_scaled.reshape((input_scaled.shape[0], 1, input_scaled.shape[1]))\n",
        "    prediction = model.predict(input_scaled)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Main function to run the application\n",
        "def main():\n",
        "    user_input = get_user_input()\n",
        "    prediction = make_prediction(user_input)\n",
        "    print(f\"Your Current Rating is: {user_input[0][0]}\")\n",
        "    print(f\"You have Participated in {user_input[0][4]} contests\")\n",
        "    print(f\"Predicted change in rating: {prediction}\")\n",
        "    print(f\"Your future rating will be: {user_input[0][0] + prediction}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YCtog77Yesm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53f777d-6b5e-44fb-bb55-9b1177de10b3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the input values:\n",
            "Enter your username: sagargupta1610\n",
            "Enter your ranking: 8000\n",
            "Enter Total Participants: 20000\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "Your Current Rating is : 1964.5613925584812\n",
            "You have Participated in 85.0 contests\n",
            "Predicted change in rating: -38.431880950927734\n",
            "your Rating will get: 1926.1295116075535\n"
          ]
        }
      ]
    }
  ]
}